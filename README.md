# Pinpointing-Jailbreaking-Defense
Large Language Models (LLMs) are increasingly deployed in critical domains such as healthcare, law, and education. However, they remain vulnerable to jailbreak attacks, where adversaries craft prompts to override alignment safeguards and force unsafe outputs.
```bash
This project provides a comprehensive framework for evaluating jailbreak attacks and defenses, combining:

(i) 16 distinct jailbreak attacks across multiple categories.
(ii) 7 defense systems including perplexity filters, rephrasing, semantic moderation, and policy guardrails.
(iii) Systematic evaluation protocol to pinpoint missing defense layers.
(iv) Extensive dataset-level success/failure analysis across thousands of adversarial queries.
```

